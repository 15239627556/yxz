# -*- coding: utf-8 -*-
# =============================================================================
# Author: yu xunzhuang
# Date: 2025/2/26
# Copyright: æ™ºå¾®ä¿¡ç§‘ (c) 2025
# License: Some License (e.g., MIT)
# =============================================================================
import json
import time

import scrapy
from bs4 import BeautifulSoup
from scrapy import Request
from scrapy.exceptions import CloseSpider
from scrapy.linkextractors import LinkExtractor
from urllib.parse import urlencode
print(f'scrapy_version: {scrapy.__version__}')

from GoogleImageSpider.items import GoogleImageItem, HrefImageItem


class GoogleImageSpider(scrapy.Spider):
    name = "googleImageSpider"  # çˆ¬è™«åç§°
    allowed_domains = ['googleapis.com', 'baidu.com']  # å…è®¸çˆ¬å–çš„åŸŸå
    domain_delay = 20  # åŒä¸€åŸŸåè¯·æ±‚é—´éš”æ—¶é—´
    redis_key = 'crawler:image_contextLink'  # Redisä»»åŠ¡é˜Ÿåˆ—é”®å
    SEARCH_QUERY = 'çº¢ç»†èƒ'  # æœç´¢å…³é”®è¯
    cx = '260207ac67ef144f4'  # æ›¿æ¢ä¸ºä½ çš„Custom Search ID
    api_key = 'AIzaSyDbOd586kF3mt1GmpBP3_T84Q01a0E-x1o'  # æ›¿æ¢ä¸ºä½ çš„APIå¯†é’¥

    # custom_settings = {
    #     'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter',
    #     'SCHEDULER': 'scrapy_redis.scheduler.Scheduler',
    #     'REDIS_URL': 'redis://localhost:6379',
    #     'MONGODB_URI': 'mongodb://localhost:27017',
    #     'MONGODB_DB': 'image_scraper',
    #     'LOG_LEVEL': 'INFO',
    #     'DOWNLOAD_DELAY': 20,
    #     'AUTOTHROTTLE_ENABLED': True,
    #     'RETRY_ENABLED': True,
    #     'RETRY_TIMES': 5,
    #     'RETRY_HTTP_CODES': [400, 403, 408, 429, 500, 502, 503, 504],
    # }

    # def __init__(self, *args, **kwargs):
    #     super().__init__(*args, **kwargs)
    #     # self.NUM_RESULTS = 10  # è·å–çš„å›¾ç‰‡æ•°é‡
    #     self.SEARCH_QUERY = 'çº¢ç»†èƒ'  # æœç´¢å…³é”®è¯
    #     self.cx = '260207ac67ef144f4'  # æ›¿æ¢ä¸ºä½ çš„Custom Search ID
    #     self.api_key = 'AIzaSyDbOd586kF3mt1GmpBP3_T84Q01a0E-x1o'  # æ›¿æ¢ä¸ºä½ çš„APIå¯†é’¥

    def start_requests(self):
        self.logger.info("âœ… Start requests triggered")
        api_url = "https://www.googleapis.com/customsearch/v1?"
        params = {
            'q': self.SEARCH_QUERY,  # searchTerms
            'cx': self.cx,  # custom search engine ID, cx
            'key': self.api_key,  # API key
            'searchType': 'image',  # æŒ‡å®šæœç´¢ç±»å‹ä¸ºå›¾ç‰‡
            'start': 1,  # startIndex
            'num': 10,  # count
            'safe': 'off',  # safe
            'inputEncoding': 'utf8',  # inputEncoding
            'outputEncoding': 'utf8'  # outputEncoding
        }
        # url = api_url + urlencode(params)
        url = 'https://www.baidu.com'
        request = scrapy.Request(url=url, callback=self.parse_href_images, errback=self.handle_error)
        # self.crawler.signals.connect(
        #     lambda response: self.logger.debug(f"ğŸ”¥ Signal: {response.status} from {response.url}"),
        #     signal=scrapy.signals.response_received
        # )
        self.logger.info(f"âœ… Start requests triggered: {url}")
        yield request

    def parse(self, response):
        print(1111)
        # è§£æ API è¿”å›çš„ JSON æ•°æ®
        data = json.loads(response.text)
        print(data)
        for item in data.get('items', []):
            image_item = GoogleImageItem()
            image_item['title'] = item.get('title')
            image_item['link'] = item.get('link')
            image_item['htmlTitle'] = item.get('htmlTitle')
            image_item['displayLink'] = item.get('displayLink')
            image_item['snippet'] = item.get('snippet')
            image_item['htmlSnippet'] = item.get('htmlSnippet')
            image_item['mime'] = item.get('mime')
            image_item['fileFormat'] = item.get('fileFormat')
            image_item['image_contextLink'] = item.get('image', {}).get('contextLink')
            image_item['image_height'] = item.get('image', {}).get('height')
            image_item['image_width'] = item.get('image', {}).get('width')
            image_item['image_byteSize'] = item.get('image', {}).get('byteSize')
            image_item['image_thumbnailLink'] = item.get('image', {}).get('thumbnailLink')
            yield image_item

            # å‘èµ·å¯¹image_contextLinkçš„è¯·æ±‚
            context_link = item.get('image', {}).get('contextLink', '')
            if context_link:
                yield scrapy.Request(
                    url=context_link,
                    callback=self.parse_href_images,
                    meta={'google_image_id': image_item['link']},
                    errback=self.handle_error
                )
        # å…ˆæ³¨é‡Šæ‰
        # å¤„ç†åˆ†é¡µ
        # next_page = data.get('queries', {}).get('nextPage', [])
        # if next_page:
        #     api_url = "https://www.googleapis.com/customsearch/v1?"
        #     params = {
        #         'q': self.SEARCH_QUERY,  # searchTerms
        #         'cx': self.cx,  # custom search engine ID, cx
        #         'key': self.api_key,  # API key
        #         'searchType': 'image',  # æŒ‡å®šæœç´¢ç±»å‹ä¸ºå›¾ç‰‡
        #         'start': next_page[0].get('startIndex', 1),  # startIndex
        #         'num': next_page[0].get('count', 10),  # count
        #         'safe': 'off',  # safe
        #         'inputEncoding': 'utf8',  # inputEncoding
        #         'outputEncoding': 'utf8'  # outputEncoding
        #     }
        #     yield scrapy.FormRequest(url=api_url + urlencode(params), callback=self.parse, errback=self.handle_error)

    def parse_href_images(self, response):
        """è§£æä» image_contextLink ä¸­æå–çš„å›¾ç‰‡åŠé€’å½’é¡µé¢é“¾æ¥"""
        # ğŸš€ è·å–å½“å‰é€’å½’æ·±åº¦ï¼ˆé¦–æ¬¡è°ƒç”¨æ—¶æ·±åº¦ä¸º0ï¼‰
        current_depth = response.meta.get("depth", 0)
        self.logger.info(f"Processing depth {current_depth}: {response.url}")

        # 1. æå–å›¾ç‰‡èµ„æºï¼ˆimgæ ‡ç­¾çš„srcï¼‰
        img_extractor = LinkExtractor(tags="img", attrs="src")
        for link in img_extractor.extract_links(response):
            href_image = HrefImageItem()
            href_image["link"] = link.url
            href_image["image_contextLink"] = response.url
            href_image["referer"] = response.request.headers.get("Referer", b"").decode("utf-8", "ignore")
            # ğŸš€ ä¼˜åŒ–å°ºå¯¸æå–é€»è¾‘ï¼ˆå»ºè®®å¼‚æ­¥ä¸‹è½½å›¾ç‰‡å¤´ä¿¡æ¯ï¼‰
            href_image["image_height"], href_image["image_height"] = self.extract_image_dimensions(link.text)
            yield href_image

        # 2. æå–é¡µé¢è¶…é“¾æ¥ï¼ˆaæ ‡ç­¾çš„hrefï¼‰ç”¨äºé€’å½’
        if current_depth < 10:  # ğŸš€ æ§åˆ¶æœ€å¤§æ·±åº¦
            page_extractor = LinkExtractor(
                tags="a",
                attrs="href",
                allow=(r"\.html$", r"\.php$"),  # ç¤ºä¾‹ï¼šä»…è·Ÿè¸ªç½‘é¡µç±»å‹é“¾æ¥
                deny_domains=("ads.com",)  # æ’é™¤å¹¿å‘ŠåŸŸå
            )
            for link in page_extractor.extract_links(response):
                # ğŸš€ é€’å½’ç”Ÿæˆæ–°è¯·æ±‚ï¼Œæ·±åº¦+1
                yield Request(
                    url=link.url,
                    callback=self.parse_href_images,
                    meta={
                        "depth": current_depth + 1,  # ğŸš€ ä¼ é€’æ·±åº¦å‚æ•°
                        "referer": response.url
                    },
                    priority=10 - current_depth  # æ·±åº¦è¶Šå¤§ä¼˜å…ˆçº§è¶Šä½
                )

    def extract_image_dimensions(self, img_tag):
        """ä»imgæ ‡ç­¾ä¸­æå–å°ºå¯¸ä¿¡æ¯ï¼ˆç¤ºä¾‹ï¼‰"""
        # ç¤ºä¾‹ï¼šsrc="image.jpg" width="300" height="200"
        width = self.extract_attr(img_tag, 'width')
        height = self.extract_attr(img_tag, 'height')
        return width, height

    def extract_attr(self, tag, attr):
        """ä»HTMLæ ‡ç­¾ä¸­æå–å±æ€§å€¼"""
        import re
        match = re.search(f'{attr}="([^"]+)"', tag)
        return match.group(1) if match else None

    def handle_error(self, failure):
        """å¤„ç†è¯·æ±‚å¤±è´¥çš„æƒ…å†µ"""
        print('å…³é—­close_spider')
        self.logger.info('error: å…³é—­close_spider')
        self.logger.error(f"Request failed: {failure}")
        # åˆ‡æ¢ä»£ç†
        self.crawler.engine.close_spider(self, "Proxy error")
        raise CloseSpider("Proxy error")