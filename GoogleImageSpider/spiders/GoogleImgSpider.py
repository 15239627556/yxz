# -*- coding: utf-8 -*-
# =============================================================================
# Author: yu xunzhuang
# Date: 2025/2/26
# Copyright: æ™ºå¾®ä¿¡ç§‘ (c) 2025
# License: Some License (e.g., MIT)
# =============================================================================
import json
from urllib.parse import urlencode, urlparse

import scrapy
from bs4 import BeautifulSoup
from scrapy import Request
from scrapy.exceptions import CloseSpider

from GoogleImageSpider.items import GoogleImageItem, HrefImageItem
from GoogleImageSpider.configuration import googleSettings


class GoogleImageSpider(scrapy.Spider):
    name = "GoogleImageSpider"
    domain_delay = 20
    redis_key = 'crawler:image_contextLink'
    SEARCH_QUERY = 'Bone Marrow Microscope'
    CX = googleSettings.get_cx()
    API_KEY = googleSettings.get_api_key()
    API_URL = "https://www.googleapis.com/customsearch/v1"

    def start_requests(self):
        self.logger.info("âœ… Start requests triggered")
        params = {
            'q': self.SEARCH_QUERY,  # searchTerms
            'cx': self.CX,  # custom search engine ID, cx
            'key': self.API_KEY,  # API key
            'searchType': 'image',  # æŒ‡å®šæœç´¢ç±»å‹ä¸ºå›¾ç‰‡
            'start': 1,  # startIndex
            'num': 10,  # count
            'safe': 'off',  # safe
            'inputEncoding': 'utf8',  # inputEncoding
            'outputEncoding': 'utf8'  # outputEncoding
        }
        new_url = f"{self.API_URL}?{urlencode(params)}"
        request = scrapy.Request(
            url=new_url,
            callback=self.parse,
            errback=self.handle_error,
            # meta={'params': params}
        )
        yield request

    def parse(self, response):
        # è§£æ API è¿”å›çš„ JSON æ•°æ®
        # æŸ¥çœ‹è¯·æ±‚çš„ç»“æœ
        data = json.loads(response.text)
        for item in data.get('items', []):
            image_item = GoogleImageItem()
            image_item['title'] = item.get('title')
            image_item['link'] = item.get('link')
            image_item['htmlTitle'] = item.get('htmlTitle')
            image_item['displayLink'] = item.get('displayLink')
            image_item['snippet'] = item.get('snippet')
            image_item['htmlSnippet'] = item.get('htmlSnippet')
            image_item['mime'] = item.get('mime')
            image_item['fileFormat'] = item.get('fileFormat')
            image_item['image_contextLink'] = item.get('image', {}).get('contextLink')
            image_item['image_height'] = item.get('image', {}).get('height')
            image_item['image_width'] = item.get('image', {}).get('width')
            image_item['image_byteSize'] = item.get('image', {}).get('byteSize')
            image_item['image_thumbnailLink'] = item.get('image', {}).get('thumbnailLink')
            image_item['category'] = "GoogleImage"
            yield image_item

            # å‘èµ·å¯¹image_contextLinkçš„è¯·æ±‚
            # æ ¹æ®é…ç½®æ–‡ä»¶ä¸­çš„DIVERGEå‚æ•°ï¼Œå†³å®šæ˜¯å¦è¿›è¡Œæ·±åº¦çˆ¬å–
            if self.settings.get('DIVERGE'):
                context_link = item.get('image', {}).get('contextLink', '')
                if context_link:
                    yield scrapy.Request(
                        url=context_link,
                        callback=self.parse_href_images,
                        meta={'google_image_id': image_item['link']},
                        errback=self.handle_error
                    )
        # å¤„ç†åˆ†é¡µ
        if self.settings.get('NEXT_PAGE'):
            next_page = data.get('queries', {}).get('nextPage', [])
            if next_page:
                params = {
                    'q': self.SEARCH_QUERY,  # searchTerms
                    'cx': self.CX,  # custom search engine ID, cx
                    'key': self.API_KEY,  # API key
                    'searchType': 'image',  # æŒ‡å®šæœç´¢ç±»å‹ä¸ºå›¾ç‰‡
                    'start': next_page[0].get('startIndex'),  # startIndex
                    'num': next_page[0].get('count', 10),  # count
                    'safe': 'off',  # safe
                    'inputEncoding': 'utf8',  # inputEncoding
                    'outputEncoding': 'utf8'  # outputEncoding
                }
                yield scrapy.Request(url=f"{self.API_URL}?{urlencode(params)}", callback=self.parse,
                                     errback=self.handle_error)

    def parse_href_images(self, response):
        """è§£æä» image_contextLink ä¸­æå–çš„å›¾ç‰‡åŠé€’å½’é¡µé¢é“¾æ¥"""
        # ğŸš€ è·å–å½“å‰é€’å½’æ·±åº¦ï¼ˆé¦–æ¬¡è°ƒç”¨æ—¶æ·±åº¦ä¸º0ï¼‰
        current_depth = response.meta.get("depth", 0)
        self.logger.info(f"Processing depth {current_depth}: {response.url}")

        # 1. æå–å›¾ç‰‡èµ„æºï¼ˆimgæ ‡ç­¾çš„srcï¼‰
        soup = BeautifulSoup(response.text, 'html.parser')
        for img in soup.find_all('img'):
            href_image = HrefImageItem()
            image_link = img.get('src')
            urlparse_url = urlparse(response.url)
            if 'http' not in image_link:
                image_link = f"{urlparse_url.scheme}://{urlparse_url.netloc}{image_link}"
            href_image["link"] = image_link
            href_image["image_contextLink"] = response.url
            href_image["referer"] = response.request.headers.get("Referer", b"").decode("utf-8", "ignore")
            href_image["image_height"], href_image["image_width"] = self.extract_image_dimensions(str(img))
            href_image['category'] = "HrefImage"
            yield href_image

        # 2. æå–é¡µé¢è¶…é“¾æ¥ï¼ˆaæ ‡ç­¾çš„hrefï¿½ï¿½ï¿½ç”¨äºé€’å½’
        if current_depth < 10:  # ğŸš€ æ§åˆ¶æœ€å¤§æ·±åº¦
            for a_tag in soup.find_all('a', href=True):
                href_link = a_tag['href']
                urlparse_href = urlparse(href_link)
                if not href_link.startswith('http'):
                    href_link = f"{urlparse_href.scheme}://{urlparse_href.netloc}{href_link}"
                # ğŸš€ é€’å½’ç”Ÿæˆæ–°è¯·æ±‚ï¼Œæ·±åº¦+1
                yield Request(
                    url=href_link,
                    callback=self.parse_href_images,
                    meta={
                        "depth": current_depth + 1,  # ğŸš€ ä¼ é€’æ·±åº¦å‚æ•°
                        "referer": response.url
                    },
                    priority=10 - current_depth  # æ·±åº¦è¶Šå¤§ä¼˜å…ˆçº§è¶Šä½
                )

    def extract_image_dimensions(self, img_tag):
        """ä»imgæ ‡ç­¾ä¸­æå–å°ºå¯¸ä¿¡æ¯ï¼ˆç¤ºä¾‹ï¼‰"""
        # ç¤ºä¾‹ï¼šsrc="image.jpg" width="300" height="200"
        width = self.extract_attr(img_tag, 'width')
        height = self.extract_attr(img_tag, 'height')
        return width, height

    def extract_attr(self, tag, attr):
        """ä»HTMLæ ‡ç­¾ä¸­æå–å±æ€§å€¼"""
        import re
        match = re.search(f'{attr}="([^"]+)"', tag)
        return match.group(1) if match else None

    def handle_error(self, failure):
        """å¤„ç†è¯·æ±‚å¤±è´¥çš„æƒ…å†µ"""
        print('å…³é—­close_spider')
        self.logger.info('error: å…³é—­close_spider')
        self.logger.error(f"Request failed: {failure}")
        # åˆ‡æ¢ä»£ç†
        self.crawler.engine.close_spider(self, "Proxy error")
        raise CloseSpider("Proxy error")
